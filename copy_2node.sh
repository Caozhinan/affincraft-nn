#!/bin/bash
#SBATCH -p gpu_4090
#SBATCH -N 2
#SBATCH --gres=gpu:8
#SBATCH --qos=gpugpu  
#SBATCH -J AffinCraft-LMDB-2nodes
#SBATCH -o /data/run01/scw6f3q/zncao/affincraft-nn/logs/pretrain_2nodes-%j.out
#SBATCH -e /data/run01/scw6f3q/zncao/affincraft-nn/logs/pretrain_2nodes-%j.err

########################################
#   ç¯å¢ƒä¸è·¯å¾„è®¾ç½®
########################################

echo "[INFO] Job starting at $(date)"

# åŠ è½½ç³»ç»Ÿæ¨¡å—
source /etc/profile.d/modules.sh
module load singularity/3.10.0
module load cuda/12.4
module load gcc/12.2

# å®¹å™¨é•œåƒè·¯å¾„
CONTAINER=~/run/12.4.1-devel-ubuntu20.04

# Conda ç¯å¢ƒè·¯å¾„åŠåˆå§‹åŒ–è„šæœ¬
ENV_PATH=/data/run01/scw6f3q/zncao/affincraft
CONDA_SH=/data/apps_4090/miniforge3/24.1.2/etc/profile.d/conda.sh

# å®é™…è®­ç»ƒè„šæœ¬è·¯å¾„
TRAIN_SCRIPT=/data/run01/scw6f3q/zncao/affincraft-nn/graphormer/train_finetune/md_multi_gpu_2nodes.sh

# æ—¥å¿—ç›®å½•
mkdir -p /data/run01/scw6f3q/zncao/affincraft-nn/logs

########################################
#   æ‰“å°ä»»åŠ¡ä¿¡æ¯
########################################
echo "=========================================="
echo " ğŸ¯ AffinCraft åŒèŠ‚ç‚¹GPUè®­ç»ƒ (LMDBæ ¼å¼)"
echo " èŠ‚ç‚¹æ•°:        $SLURM_NNODES"
echo " èŠ‚ç‚¹åˆ—è¡¨:      $SLURM_JOB_NODELIST"
echo " ä½œä¸šID:        $SLURM_JOB_ID"
echo " æ€»GPUæ•°:       16 (8Ã—2)"
echo " å¯åŠ¨æ—¶é—´:      $(date)"
echo "=========================================="

########################################
#   åˆ†å¸ƒå¼å®¹å™¨æ‰§è¡Œé€»è¾‘
########################################
# srun ä¼šç¡®ä¿åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šå„å¯åŠ¨ä¸€ä¸ªè¿›ç¨‹
srun --ntasks=$SLURM_NNODES --ntasks-per-node=1 \
    singularity exec --nv \
    --bind /data/run01/scw6f3q:/data/run01/scw6f3q \
    --bind /data/apps_4090:/data/apps_4090 \
    --bind /ssd/home/scw6f3q:/ssd/home/scw6f3q \
    $CONTAINER \
    bash -c "
        set -euo pipefail
        source $CONDA_SH
        conda activate $ENV_PATH
        export PYTHONUNBUFFERED=1
        export PYTHONWARNINGS='ignore::UserWarning:pkg_resources, ignore::FutureWarning:dgl.backend.pytorch.sparse, ignore::FutureWarning, ignore::UserWarning'

        echo '[INFO] å½“å‰èŠ‚ç‚¹:' \$(hostname)
        echo '[INFO] Python:' \$(which python)
        python -c 'import torch; print(\"[INFO] Torch version:\", torch.__version__); print(\"[INFO] CUDA available:\", torch.cuda.is_available()); print(\"[INFO] GPU count:\", torch.cuda.device_count())'

        echo '[INFO] å¼€å§‹æ‰§è¡Œè®­ç»ƒè„šæœ¬: $TRAIN_SCRIPT'
        bash $TRAIN_SCRIPT
    "

########################################
#   æ”¶å°¾ä¿¡æ¯è¾“å‡º
########################################
echo "=========================================="
echo " âœ… ä»»åŠ¡å®Œæˆäº: $(date)"
echo "=========================================="

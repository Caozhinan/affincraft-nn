#!/usr/bin/env python3  
"""  
æ¸…ç† LMDB æ•°æ®åº“ä¸­åŒ…å« NaN çš„æ ·æœ¬  
æ£€æŸ¥æ‰€æœ‰å­—æ®µ: node_feat, edge_index, edge_feat, coords, angle, dists, attn_bias, masif_features  
"""  
  
import pickle  
import lmdb  
import numpy as np  
import torch  
from pathlib import Path  
import sys  
from tqdm import tqdm  
  
  
def has_nan_in_sample(sample):  
    """å…¨é¢æ£€æŸ¥æ ·æœ¬ä¸­æ‰€æœ‰å­—æ®µæ˜¯å¦åŒ…å« NaN"""  
    # éœ€è¦æ£€æŸ¥çš„æµ®ç‚¹æ•°å­—æ®µ  
    float_fields = ['edge_feat', 'coords', 'angle', 'dists', 'attn_bias']  
      
    for field in float_fields:  
        if field not in sample:  
            continue  
              
        data = sample[field]  
          
        if isinstance(data, torch.Tensor):  
            if data.dtype in [torch.float32, torch.float64, torch.float16]:  
                if torch.isnan(data).any().item():  
                    return True  
        elif isinstance(data, np.ndarray):  
            if np.issubdtype(data.dtype, np.floating):  
                if np.isnan(data).any():  
                    return True  
      
    # æ£€æŸ¥ masif_features  
    if 'masif_features' in sample:  
        masif_data = sample['masif_features']  
        if isinstance(masif_data, dict):  
            for key, value in masif_data.items():  
                if isinstance(value, torch.Tensor):  
                    if value.dtype in [torch.float32, torch.float64, torch.float16]:  
                        if torch.isnan(value).any().item():  
                            return True  
                elif isinstance(value, np.ndarray):  
                    if np.issubdtype(value.dtype, np.floating):  
                        if np.isnan(value).any():  
                            return True  
      
    # æ£€æŸ¥å…¶ä»–æµ®ç‚¹æ•°å­—æ®µ  
    for key, value in sample.items():  
        if isinstance(value, (float, np.floating)):  
            if np.isnan(value):  
                return True  
      
    return False  
  
  
def clean_lmdb(source_lmdb, target_lmdb, map_size=int(4e12)):  
    """æ¸…ç† LMDB æ•°æ®åº“,ç§»é™¤åŒ…å« NaN çš„æ ·æœ¬"""  
    source_path = Path(source_lmdb).expanduser().absolute()  
    target_path = Path(target_lmdb).expanduser().absolute()  
      
    if not source_path.exists():  
        raise FileNotFoundError(f"âŒ æº LMDB ä¸å­˜åœ¨: {source_path}")  
      
    if target_path.exists():  
        response = input(f"âš ï¸  ç›®æ ‡è·¯å¾„å·²å­˜åœ¨: {target_path}\næ˜¯å¦è¦†ç›–? (y/n): ")  
        if response.lower() != 'y':  
            print("æ“ä½œå·²å–æ¶ˆ")  
            return  
        import shutil  
        shutil.rmtree(target_path)  
      
    print(f"=" * 60)  
    print(f"ğŸ§¹ å¼€å§‹æ¸…ç† LMDB æ•°æ®åº“")  
    print(f"æºè·¯å¾„: {source_path}")  
    print(f"ç›®æ ‡è·¯å¾„: {target_path}")  
    print(f"=" * 60)  
      
    # æ‰“å¼€æº LMDB  
    source_env = lmdb.open(  
        str(source_path),  
        readonly=True,  
        lock=False,  
        readahead=False,  
        meminit=False,  
        max_readers=256  
    )  
      
    # è¯»å–åŸå§‹å…ƒæ•°æ®  
    original_metadata = None  
    with source_env.begin() as txn:  
        metadata_bytes = txn.get(b'__metadata__')  
        if metadata_bytes is None:  
            print("âŒ æº LMDB ç¼ºå°‘å…ƒæ•°æ®")  
            source_env.close()  
            return  
          
        original_metadata = pickle.loads(metadata_bytes)  
        total_samples = original_metadata.get('total_samples', 0)  
        print(f"æºæ•°æ®åº“æ ·æœ¬æ€»æ•°: {total_samples:,}")  
        print(f"åŸå§‹å…ƒæ•°æ®å­—æ®µ: {list(original_metadata.keys())}\n")  
      
    # åˆ›å»ºç›®æ ‡ LMDB  
    target_env = lmdb.open(  
        str(target_path),  
        map_size=map_size,  
        subdir=True,  
        readonly=False,  
        lock=True,  
        metasync=False,  
        sync=False,  
        map_async=True,  
        writemap=True,  
        meminit=False,  
        max_readers=1  
    )  
      
    valid_count = 0  
    nan_count = 0  
    error_count = 0  
    total_size = 0  
    nan_pdbids = []  
    nan_fields_stats = {}  # ç»Ÿè®¡å“ªäº›å­—æ®µåŒ…å« NaN  
      
    print("å¼€å§‹å¤„ç†æ ·æœ¬...")  
      
    with source_env.begin() as source_txn:  
        target_txn = target_env.begin(write=True)  
          
        try:  
            for idx in tqdm(range(total_samples), desc="å¤„ç†è¿›åº¦"):  
                try:  
                    key = f'{idx}'.encode('ascii')  
                    data_bytes = source_txn.get(key)  
                      
                    if data_bytes is None:  
                        error_count += 1  
                        continue  
                      
                    sample = pickle.loads(data_bytes)  
                      
                    # æ£€æŸ¥ NaN  
                    if has_nan_in_sample(sample):  
                        pdbid = sample.get('pdbid', f'sample_{idx}')  
                        nan_pdbids.append(pdbid)  
                        nan_count += 1  
                          
                        # ç»Ÿè®¡å“ªä¸ªå­—æ®µæœ‰ NaN (ç”¨äºè°ƒè¯•)  
                        for field in ['edge_feat', 'coords', 'angle', 'dists', 'attn_bias']:  
                            if field in sample:  
                                data = sample[field]  
                                has_nan = False  
                                if isinstance(data, torch.Tensor):  
                                    if data.dtype in [torch.float32, torch.float64, torch.float16]:  
                                        has_nan = torch.isnan(data).any().item()  
                                elif isinstance(data, np.ndarray):  
                                    if np.issubdtype(data.dtype, np.floating):  
                                        has_nan = np.isnan(data).any()  
                                  
                                if has_nan:  
                                    nan_fields_stats[field] = nan_fields_stats.get(field, 0) + 1  
                          
                        if nan_count % 100 == 0:  
                            print(f"\nå·²è·³è¿‡ {nan_count} ä¸ªåŒ…å« NaN çš„æ ·æœ¬")  
                        continue  
                      
                    # å†™å…¥ç›®æ ‡ LMDB  
                    new_key = f'{valid_count}'.encode('ascii')  
                    target_txn.put(new_key, data_bytes)  
                      
                    total_size += len(data_bytes)  
                    valid_count += 1  
                      
                    # å®šæœŸæäº¤äº‹åŠ¡  
                    if valid_count % 10000 == 0:  
                        target_txn.commit()  
                        target_txn = target_env.begin(write=True)  
                  
                except Exception as e:  
                    print(f"\nâŒ å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")  
                    error_count += 1  
                    continue  
          
        finally:  
            target_txn.commit()  
      
    # å†™å…¥æ–°çš„å…ƒæ•°æ® (ä¿ç•™åŸæœ‰æ ¼å¼)  
    new_metadata = original_metadata.copy()  
    new_metadata['total_samples'] = valid_count  
    new_metadata['total_size_bytes'] = total_size  
    new_metadata['cleaned_from'] = str(source_path)  
    new_metadata['removed_nan_samples'] = nan_count  
    new_metadata['removed_error_samples'] = error_count  
    new_metadata['nan_pdbids_sample'] = nan_pdbids[:100]  
    new_metadata['nan_fields_stats'] = nan_fields_stats  
      
    with target_env.begin(write=True) as txn:  
        txn.put(b'__metadata__', pickle.dumps(new_metadata))  
      
    source_env.close()  
    target_env.close()  
      
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯  
    print(f"\n{'=' * 60}")  
    print(f"âœ… æ¸…ç†å®Œæˆ!")  
    print(f"{'=' * 60}")  
    print(f"åŸå§‹æ ·æœ¬æ•°: {total_samples:,}")  
    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {valid_count:,}")  
    print(f"ç§»é™¤ NaN æ ·æœ¬: {nan_count:,} ({nan_count/total_samples*100:.2f}%)")  
    print(f"ç§»é™¤é”™è¯¯æ ·æœ¬: {error_count:,}")  
    print(f"æ•°æ®å¤§å°: {total_size / (1024**3):.2f} GB")  
    print(f"è¾“å‡ºè·¯å¾„: {target_path}")  
    print(f"\nä¿ç•™çš„åŸå§‹å…ƒæ•°æ®å­—æ®µ: {list(original_metadata.keys())}")  
      
    if nan_fields_stats:  
        print(f"\nNaN å­—æ®µç»Ÿè®¡:")  
        for field, count in sorted(nan_fields_stats.items(), key=lambda x: x[1], reverse=True):  
            print(f"  {field}: {count} ä¸ªæ ·æœ¬")  
      
    if nan_pdbids:  
        print(f"\nå‰ 20 ä¸ªè¢«ç§»é™¤çš„æ ·æœ¬ pdbid:")  
        for pdbid in nan_pdbids[:20]:  
            print(f"  - {pdbid}")  
        if len(nan_pdbids) > 20:  
            print(f"  ... è¿˜æœ‰ {len(nan_pdbids) - 20} ä¸ª")  
  
  
def main():  
    if len(sys.argv) != 3:  
        print("ç”¨æ³•:")  
        print("  python clean_lmdb_nan.py <source_lmdb> <target_lmdb>")  
        print("\nç¤ºä¾‹:")  
        print("  python clean_lmdb_nan.py /ssd/home/scw6f3q/train_lmdb /ssd/home/scw6f3q/new_train_lmdb")  
        sys.exit(1)  
      
    source_lmdb = sys.argv[1]  
    target_lmdb = sys.argv[2]  
      
    try:  
        clean_lmdb(source_lmdb, target_lmdb)  
    except Exception as e:  
        print(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")  
        import traceback  
        traceback.print_exc()  
        sys.exit(1)  
  
  
if __name__ == "__main__":  
    main()
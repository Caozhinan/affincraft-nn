import pickle    
import torch    
import numpy as np    
from pathlib import Path    
from typing import List, Dict, Any, Optional    
from torch.utils.data import Dataset, DataLoader    
import mmap
import os 
from typing import Optional
import lmdb
import time
# å¦‚æœéœ€è¦ä½¿ç”¨ç°æœ‰çš„æ•°æ®å¤„ç†å·¥å…·    
# from .wrapper import preprocess_item    
# from .collator import collator  
  
class AffinCraftDataset(torch.utils.data.Dataset):      
    def __init__(self, data, is_merged=False):    
        if is_merged:    
            # ç›´æ¥ä½¿ç”¨å¤åˆç‰©æ•°æ®åˆ—è¡¨    
            self.complexes = data    
            self.pkl_files = None    
        else:    
            # åŸæœ‰çš„PKLæ–‡ä»¶åˆ—è¡¨æ–¹å¼    
            self.pkl_files = data    
            self.complexes = None    
              
    def __len__(self):    
        if self.complexes is not None:    
            return len(self.complexes)    
        return len(self.pkl_files)    
          
    def __getitem__(self, idx):      
        if self.complexes is not None:      
            # ç›´æ¥ä»å¤åˆç‰©åˆ—è¡¨è·å–æ•°æ®ï¼ˆå·²ä¿®å¤å­—å…¸è®¿é—®é—®é¢˜ï¼‰  
            pkl_data = self.complexes[idx]      
        else:      
            # åŸæœ‰æ–¹å¼ï¼šä»PKLæ–‡ä»¶åŠ è½½      
            pkl_file = self.pkl_files[idx]        
            with open(pkl_file, 'rb') as f:        
                pkl_data = pickle.load(f)[0]    
  
        return preprocess_affincraft_item(pkl_data)  
  
  
class BatchedLazyAffinCraftDataset(torch.utils.data.Dataset):    
    def __init__(self, pkl_file_path, batch_size=32):    
        self.pkl_file_path = pkl_file_path    
        self.batch_size = batch_size    
        self._length = None    
        self._file_positions = None    
        self._batch_cache = {}  # ç¼“å­˜å·²åŠ è½½çš„æ‰¹æ¬¡    
        self._build_index()    
        
    def _build_index(self):    
        """æ„å»ºæ–‡ä»¶ä½ç½®ç´¢å¼•ï¼Œä¸åŠ è½½å®é™…æ•°æ®"""    
        positions = []    
        with open(self.pkl_file_path, 'rb') as f:    
            try:    
                while True:    
                    pos = f.tell()    
                    pickle.load(f)  # è·³è¿‡å¯¹è±¡    
                    positions.append(pos)    
            except EOFError:    
                pass    
            
        self._file_positions = positions    
        self._length = len(positions)    
        
    def _get_batch_indices(self, idx):    
        """è·å–åŒ…å«æŒ‡å®šç´¢å¼•çš„æ‰¹æ¬¡èŒƒå›´"""    
        batch_id = idx // self.batch_size    
        start_idx = batch_id * self.batch_size    
        end_idx = min(start_idx + self.batch_size, self._length)    
        return batch_id, start_idx, end_idx    
        
    def _load_batch(self, batch_id, start_idx, end_idx):    
        """åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""    
        if batch_id in self._batch_cache:    
            return self._batch_cache[batch_id]    
            
        batch_data = []    
        with open(self.pkl_file_path, 'rb') as f:    
            for i in range(start_idx, end_idx):    
                f.seek(self._file_positions[i])    
                pkl_data = pickle.load(f)    
                batch_data.append(pkl_data)    
            
        # ç¼“å­˜æ‰¹æ¬¡æ•°æ®ï¼ˆå¯é€‰ï¼šé™åˆ¶ç¼“å­˜å¤§å°ï¼‰    
        if len(self._batch_cache) < 1000:  # æœ€å¤šç¼“å­˜10ä¸ªæ‰¹æ¬¡    
            self._batch_cache[batch_id] = batch_data    
            
        return batch_data    
        
    def __len__(self):    
        return self._length    
        
    def __getitem__(self, idx):    
        batch_id, start_idx, end_idx = self._get_batch_indices(idx)    
        batch_data = self._load_batch(batch_id, start_idx, end_idx)    
            
        # è¿”å›æ‰¹æ¬¡ä¸­çš„ç‰¹å®šæ ·æœ¬    
        local_idx = idx - start_idx    
        pkl_data = batch_data[local_idx]    
            
        return preprocess_affincraft_item(pkl_data)  

  
class LMDBAffinCraftDataset(torch.utils.data.Dataset):  
    """  
    åŸºäºLMDBçš„AffinCraftæ•°æ®é›†  
    æ”¯æŒé«˜æ•ˆçš„éšæœºè®¿é—®å’Œå¤šè¿›ç¨‹å¹¶å‘è¯»å–  
    ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–é¿å…forké—®é¢˜  
    """  
      
    def __init__(self, lmdb_path: str, readonly: bool = True):  
        """  
        åˆå§‹åŒ–LMDBæ•°æ®é›†  
          
        Args:  
            lmdb_path: LMDBæ•°æ®åº“è·¯å¾„  
            readonly: æ˜¯å¦ä»¥åªè¯»æ¨¡å¼æ‰“å¼€(è®­ç»ƒæ—¶åº”ä¸ºTrue)  
        """  
        self.lmdb_path = Path(lmdb_path).expanduser().absolute()  
        self.readonly = readonly  
          
        if not self.lmdb_path.exists():  
            raise FileNotFoundError(f"LMDBè·¯å¾„ä¸å­˜åœ¨: {self.lmdb_path}")  
          
        # ğŸŸ¡ å»¶è¿Ÿåˆå§‹åŒ–:ä¸åœ¨__init__ä¸­æ‰“å¼€LMDB  
        self.env = None  
        self._length = None  
          
        # åªåœ¨ä¸»è¿›ç¨‹ä¸­è¯»å–å…ƒæ•°æ®è·å–æ ·æœ¬æ€»æ•°  
        self._load_metadata()  
          
        print(f"LMDBæ•°æ®é›†åŠ è½½å®Œæˆ: {self.lmdb_path}")  
        print(f"æ ·æœ¬æ€»æ•°: {self._length:,}")  
      
    def _load_metadata(self):  
        """åªè¯»å–å…ƒæ•°æ®,ä¸ä¿æŒç¯å¢ƒæ‰“å¼€"""  
        env = lmdb.open(  
            str(self.lmdb_path),  
            readonly=True,  
            lock=False,  
            readahead=False,  
            meminit=False,  
            max_readers=256  
        )  
          
        with env.begin() as txn:  
            metadata_bytes = txn.get(b'__metadata__')  
            if metadata_bytes is None:  
                raise ValueError(f"LMDBæ•°æ®åº“ç¼ºå°‘å…ƒæ•°æ®: {self.lmdb_path}")  
              
            metadata = pickle.loads(metadata_bytes)  
            self._length = metadata['total_samples']  
          
        env.close()  # ç«‹å³å…³é—­,ä¸ä¿æŒæ‰“å¼€  
      
    def _init_db(self):  
        """å»¶è¿Ÿåˆå§‹åŒ–:åœ¨ç¬¬ä¸€æ¬¡è®¿é—®æ—¶æ‰æ‰“å¼€LMDB"""  
        if self.env is None:  
            self.env = lmdb.open(  
                str(self.lmdb_path),  
                readonly=self.readonly,  
                lock=False,  
                readahead=False,  
                meminit=False,  
                max_readers=256  
            )  
      
    def __len__(self):  
        return self._length  
      
    def __getitem__(self, idx):  
        """  
        è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬,è‡ªåŠ¨è·³è¿‡åŒ…å«NaNçš„æ ·æœ¬  
          
        Args:  
            idx: æ ·æœ¬ç´¢å¼•  
              
        Returns:  
            é¢„å¤„ç†åçš„æ ·æœ¬æ•°æ®,å¦‚æœåŒ…å«NaNåˆ™è¿”å›None  
        """  
        if idx >= self._length or idx < 0:  
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {self._length})")  
          
        # ğŸŸ¡ å»¶è¿Ÿåˆå§‹åŒ–:æ¯ä¸ªworkerè¿›ç¨‹ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰æ‰“å¼€LMDB  
        self._init_db()  
          
        # ä»LMDBè¯»å–æ•°æ®  
        with self.env.begin() as txn:  
            key = f'{idx}'.encode('ascii')  
            data_bytes = txn.get(key)  
              
            if data_bytes is None:  
                raise RuntimeError(f"æ— æ³•è¯»å–ç´¢å¼• {idx} çš„æ•°æ®")  
              
            # ååºåˆ—åŒ–  
            pkl_data = pickle.loads(data_bytes)  
          
        # ä½¿ç”¨ç°æœ‰çš„é¢„å¤„ç†å‡½æ•°(å·²åŒ…å«NaNæ£€æµ‹)  
        processed_data = preprocess_affincraft_item(pkl_data)  
          
        # å¦‚æœè¿”å›None,è¯´æ˜åŒ…å«NaN  
        if processed_data is None:  
            pdbid = pkl_data.get('pdbid', f'sample_{idx}')  
            print(f"è­¦å‘Š: è·³è¿‡åŒ…å«NaNçš„æ ·æœ¬ {idx} (pdbid: {pdbid})")  
            return None  
          
        return processed_data  
      
    def __del__(self):  
        """æ¸…ç†èµ„æº"""  
        if hasattr(self, 'env') and self.env is not None:  
            self.env.close()
  
  
class CachedLMDBAffinCraftDataset(torch.utils.data.Dataset):  
    """  
    å¸¦ç¼“å­˜çš„LMDBæ•°æ®é›†  
    åœ¨å†…å­˜ä¸­ç¼“å­˜æœ€è¿‘è®¿é—®çš„æ ·æœ¬,è¿›ä¸€æ­¥æå‡æ€§èƒ½  
    """  
      
    def __init__(self, lmdb_path: str, cache_size: int = 1000, readonly: bool = True):  
        """  
        åˆå§‹åŒ–å¸¦ç¼“å­˜çš„LMDBæ•°æ®é›†  
          
        Args:  
            lmdb_path: LMDBæ•°æ®åº“è·¯å¾„  
            cache_size: ç¼“å­˜æ ·æœ¬æ•°é‡  
            readonly: æ˜¯å¦ä»¥åªè¯»æ¨¡å¼æ‰“å¼€  
        """  
        self.lmdb_path = Path(lmdb_path).expanduser().absolute()  
        self.cache_size = cache_size  
        self._cache = {}  # ç®€å•çš„å­—å…¸ç¼“å­˜  
          
        if not self.lmdb_path.exists():  
            raise FileNotFoundError(f"LMDBè·¯å¾„ä¸å­˜åœ¨: {self.lmdb_path}")  
          
        # æ‰“å¼€LMDBç¯å¢ƒ  
        self.env = lmdb.open(  
            str(self.lmdb_path),  
            readonly=readonly,  
            lock=False,  
            readahead=False,  
            meminit=False,  
            max_readers=256  
        )  
          
        # è¯»å–å…ƒæ•°æ®  
        with self.env.begin() as txn:  
            metadata_bytes = txn.get(b'__metadata__')  
            if metadata_bytes is None:  
                raise ValueError(f"LMDBæ•°æ®åº“ç¼ºå°‘å…ƒæ•°æ®: {self.lmdb_path}")  
              
            metadata = pickle.loads(metadata_bytes)  
            self._length = metadata['total_samples']  
          
        print(f"å¸¦ç¼“å­˜çš„LMDBæ•°æ®é›†åŠ è½½å®Œæˆ: {self.lmdb_path}")  
        print(f"æ ·æœ¬æ€»æ•°: {self._length:,}, ç¼“å­˜å¤§å°: {cache_size}")  
      
    def __len__(self):  
        return self._length  
      
    def __getitem__(self, idx):  
        if idx >= self._length or idx < 0:  
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {self._length})")  
          
        # æ£€æŸ¥ç¼“å­˜  
        if idx in self._cache:  
            return self._cache[idx]  
          
        # ä»LMDBè¯»å–  
        with self.env.begin() as txn:  
            key = f'{idx}'.encode('ascii')  
            data_bytes = txn.get(key)  
              
            if data_bytes is None:  
                raise RuntimeError(f"æ— æ³•è¯»å–ç´¢å¼• {idx} çš„æ•°æ®")  
              
            pkl_data = pickle.loads(data_bytes)  
          
        # é¢„å¤„ç†  
        processed_data = preprocess_affincraft_item(pkl_data)  
          
        # æ›´æ–°ç¼“å­˜(ç®€å•çš„LRUç­–ç•¥)  
        if len(self._cache) >= self.cache_size:  
            # åˆ é™¤ç¬¬ä¸€ä¸ªå…ƒç´ (æœ€æ—§çš„)  
            self._cache.pop(next(iter(self._cache)))  
          
        self._cache[idx] = processed_data  
          
        return processed_data  
      
    def __del__(self):  
        if hasattr(self, 'env'):  
            self.env.close()

class OptimizedBatchedLazyAffinCraftDataset(torch.utils.data.Dataset):  
    def __init__(self, pkl_file_path, batch_size=16, total_objects=None, index_file_path=None):  
        """  
        ä¼˜åŒ–çš„æ‰¹æ¬¡æ‡’åŠ è½½æ•°æ®é›†  
          
        Args:  
            pkl_file_path: PKLæ–‡ä»¶è·¯å¾„  
            batch_size: æ‰¹æ¬¡å¤§å°  
            total_objects: æ–‡ä»¶ä¸­çš„æ€»å¯¹è±¡æ•°é‡ï¼ˆå¯é€‰ï¼Œä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰  
            index_file_path: é¢„æ„å»ºç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæä¾›åç›´æ¥ä½¿ç”¨ç²¾ç¡®ä½ç½®ï¼‰  
        """  
        self.pkl_file_path = pkl_file_path  
        self.batch_size = batch_size  
        self.total_objects = total_objects  
        self.file_size = os.path.getsize(pkl_file_path)  
          
        self._length = None  
        self._file_positions = None  
        self._batch_cache = {}  
          
        print(f"æ–‡ä»¶å¤§å°: {self.file_size / (1024**3):.2f} GB")  
          
        # ä¼˜å…ˆä½¿ç”¨é¢„æ„å»ºç´¢å¼•ï¼Œå¦åˆ™æ„å»ºå®Œæ•´ç´¢å¼•  
        if index_file_path and os.path.exists(index_file_path):  
            self._load_prebuilt_index(index_file_path)  
        else:  
            print("æœªæä¾›ç´¢å¼•æ–‡ä»¶ï¼Œå¼€å§‹æ„å»ºå®Œæ•´ç´¢å¼•...")  
            self._build_full_index()  
  
    def _load_prebuilt_index(self, index_file_path):  
        """åŠ è½½é¢„æ„å»ºçš„ç´¢å¼•æ–‡ä»¶"""  
        print(f"åŠ è½½é¢„æ„å»ºç´¢å¼•æ–‡ä»¶: {index_file_path}")  
          
        with open(index_file_path, 'rb') as f:  
            index_data = pickle.load(f)  
          
        self._file_positions = index_data['positions']  
        self._length = index_data['total_objects']  
          
        # éªŒè¯ç´¢å¼•æ–‡ä»¶ä¸PKLæ–‡ä»¶çš„ä¸€è‡´æ€§  
        if index_data.get('file_size') != self.file_size:  
            print(f"è­¦å‘Š: ç´¢å¼•æ–‡ä»¶è®°å½•çš„æ–‡ä»¶å¤§å°({index_data.get('file_size')})ä¸å®é™…æ–‡ä»¶å¤§å°({self.file_size})ä¸åŒ¹é…")  
          
        print(f"æˆåŠŸåŠ è½½é¢„æ„å»ºç´¢å¼•ï¼š{self._length:,} ä¸ªå¯¹è±¡")  
  
    def _build_full_index(self):    
        """æ„å»ºå®Œæ•´çš„ä½ç½®ç´¢å¼• - é€ä¸ªåŠ è½½æ‰€æœ‰å¯¹è±¡"""    
        positions = []    
    
        with open(self.pkl_file_path, 'rb') as f:    
            count = 0    
            while True:    
                try:  
                    pos = f.tell()    
                    pickle.load(f)  # åŠ è½½å¯¹è±¡ä»¥ç§»åŠ¨æ–‡ä»¶æŒ‡é’ˆ    
                    positions.append(pos)    
                    count += 1    
    
                    if count % 10000 == 0:    
                        print(f"å·²ç´¢å¼• {count:,} ä¸ªå¯¹è±¡...")    
    
                except EOFError:  
                    # æ­£å¸¸çš„æ–‡ä»¶ç»“æŸ  
                    print(f"å®Œæ•´ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {count:,} ä¸ªå¯¹è±¡")  
                    break  

                except pickle.UnpicklingError as e:    
                    print(f"è­¦å‘Šï¼šåœ¨ä½ç½® {pos} é‡åˆ°pickleé”™è¯¯: {e}")    
                    print(f"å¯èƒ½çš„æ–‡ä»¶æˆªæ–­ï¼Œåœæ­¢ç´¢å¼•æ„å»º")    
                    break    
                
        self._file_positions = positions    
        self._length = len(positions)    

        if len(positions) == 0:    
            raise RuntimeError(f"æ— æ³•ä»PKLæ–‡ä»¶ä¸­è¯»å–ä»»ä½•æœ‰æ•ˆå¯¹è±¡: {self.pkl_file_path}")
  
    def _get_batch_indices(self, idx):  
        """è·å–åŒ…å«æŒ‡å®šç´¢å¼•çš„æ‰¹æ¬¡èŒƒå›´"""  
        batch_id = idx // self.batch_size  
        start_idx = batch_id * self.batch_size  
        end_idx = min(start_idx + self.batch_size, self._length)  
        return batch_id, start_idx, end_idx  
  
    def _load_batch_from_positions(self, batch_id, start_idx, end_idx):  
        """ä»ç²¾ç¡®ä½ç½®åŠ è½½æ‰¹æ¬¡æ•°æ®"""  
        if batch_id in self._batch_cache:  
            return self._batch_cache[batch_id]  
          
        batch_data = []  
        with open(self.pkl_file_path, 'rb') as f:  
            for i in range(start_idx, end_idx):  
                try:  
                    pos = self._file_positions[i]  # ç›´æ¥ä½¿ç”¨ç²¾ç¡®ä½ç½®  
                    f.seek(pos)  
                    pkl_data = pickle.load(f)  
                    batch_data.append(pkl_data)  
                except (EOFError, pickle.UnpicklingError, OSError) as e:  
                    print(f"Warning: Failed to load object at index {i}: {str(e)}")  
                    if isinstance(e, EOFError):  
                        break  
                    continue  
                except Exception as e:  
                    print(f"Unexpected error loading object at index {i}: {str(e)}")  
                    continue  
          
        # ç¼“å­˜æ‰¹æ¬¡æ•°æ®ï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰  
        if len(self._batch_cache) < 10 and batch_data:  
            self._batch_cache[batch_id] = batch_data  
          
        return batch_data  
  
    def __len__(self):  
        return self._length  
  
    def __getitem__(self, idx):  
        if idx >= self._length:  
            raise IndexError(f"Index {idx} out of range for dataset of size {self._length}")  
          
        try:
            start = time.time()  
            batch_id, start_idx, end_idx = self._get_batch_indices(idx)  
            batch_data = self._load_batch_from_positions(batch_id, start_idx, end_idx)  
              
            local_idx = idx - start_idx  
            if local_idx < len(batch_data) and batch_data[local_idx] is not None:  
                pkl_data = batch_data[local_idx]  
            elif batch_data:  
                pkl_data = batch_data[-1]  
            else:  
                raise RuntimeError(f"Batch data is empty for index {idx}, batch_id {batch_id}")  
              
            if pkl_data is None:  
                raise RuntimeError(f"pkl_data is None at index {idx}")  
            print("one data time is ",time.time()-start )  
            return preprocess_affincraft_item(pkl_data)  
              
        except Exception as e:  
            print(f"Error loading data at index {idx}: {str(e)}")  
            raise RuntimeError(f"Failed to load data at index {idx}: {str(e)}")   


def preprocess_affincraft_item(pkl_data):        
    """ä¸“é—¨å¤„ç†AffinCraft PKLæ–‡ä»¶çš„é¢„å¤„ç†å‡½æ•°"""  
      
    # æ·»åŠ  NaN æ£€æµ‹ - æ£€æŸ¥æ‰€æœ‰å…³é”®çš„æµ®ç‚¹æ•°ç»„å­—æ®µ  
    critical_fields = ['node_feat', 'edge_feat', 'coords', 'masif_desc_straight']  
      
    for field in critical_fields:  
        if field in pkl_data:  
            data = pkl_data[field]  
            if isinstance(data, np.ndarray) and np.issubdtype(data.dtype, np.floating):  
                if np.isnan(data).any():  
                    # è·³è¿‡åŒ…å« NaN çš„æ ·æœ¬  
                    return None  
    # åœ¨ preprocess_affincraft_item ä¸­æ·»åŠ   
    if pkl_data['num_node'][0] <= 0:  
        print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„ num_ligand_atoms: {pkl_data['num_node'][0]} (pdbid: {pkl_data['pdbid']})")  
        return None
    # åŸæœ‰çš„é¢„å¤„ç†é€»è¾‘        
    node_feat = torch.from_numpy(pkl_data['node_feat'])        
    edge_index = torch.from_numpy(pkl_data['edge_index'])        
    edge_feat = torch.from_numpy(pkl_data['edge_feat'])        
    coords = torch.from_numpy(pkl_data['coords'])        
        
    # ç”Ÿæˆè§’åº¦å’Œè·ç¦»ç‰¹å¾    
    from .wrapper import gen_angle_dist      
    item_data = {      
        'edge_index': edge_index,      
        'pos': coords      
    }      
    angle, dists = gen_angle_dist(item_data)     
        
    # å¤„ç†åˆ†ç¦»çš„ç©ºé—´è¾¹ä¿¡æ¯        
    lig_spatial_edges = {        
        'index': torch.from_numpy(pkl_data['lig_spatial_edge_index']),        
        'attr': torch.from_numpy(pkl_data['lig_spatial_edge_attr'])        
    }        
            
    pro_spatial_edges = {        
        'index': torch.from_numpy(pkl_data['pro_spatial_edge_index']),        
        'attr': torch.from_numpy(pkl_data['pro_spatial_edge_attr'])        
    }        
            
    # ä¿®æ”¹ï¼šåªä¿ç•™å­˜åœ¨çš„MaSIFç‰¹å¾      
    masif_features = {}      
    if 'masif_desc_straight' in pkl_data:      
        masif_features['desc_straight'] = torch.from_numpy(pkl_data['masif_desc_straight'])      
          
    # å¯é€‰ï¼šå¦‚æœéœ€è¦å…¶ä»–MaSIFç‰¹å¾ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨      
    for key in ['masif_input_feat', 'masif_desc_flipped', 'masif_rho_wrt_center',       
                'masif_theta_wrt_center', 'masif_mask']:      
        if key in pkl_data:      
            dict_key = key.replace('masif_', '')      
            masif_features[dict_key] = torch.from_numpy(pkl_data[key])      
            
    # æ·»åŠ embeddingå±‚éœ€è¦çš„é¢å¤–å­—æ®µ        
    N = node_feat.shape[0]        
            
    # åˆ›å»ºåŸºç¡€æ³¨æ„åŠ›åç½®çŸ©é˜µ        
    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)        
            
    # è®¡ç®—åº¦æ•°ä¿¡æ¯ï¼ˆä»è¾¹ç´¢å¼•è®¡ç®—ï¼‰        
    adj = torch.zeros([N, N], dtype=torch.bool)        
    adj[edge_index[0, :], edge_index[1, :]] = True        
    in_degree = adj.long().sum(dim=1).view(-1)        
    out_degree = in_degree  # å¯¹äºæ— å‘å›¾        
            
    return {        
        'node_feat': node_feat,        
        'edge_index': edge_index,        
        'edge_feat': edge_feat,        
        'coords': coords,        
        'lig_spatial_edges': lig_spatial_edges,        
        'pro_spatial_edges': pro_spatial_edges,        
        'masif_features': masif_features,        
        'num_ligand_atoms': pkl_data['num_node'][0],        
        'attn_bias': attn_bias,        
        'in_degree': in_degree,        
        'out_degree': out_degree,        
        'gbscore': torch.from_numpy(pkl_data['gbscore']),        
        'pdbid': pkl_data['pdbid'],    
        'angle': angle,    
        'dists': dists,         
        'pk': pkl_data['pk'],        
        'smiles': pkl_data['smiles'],      
        'rmsd': pkl_data['rmsd']      
    }    
  
def affincraft_collator(items, max_node=512):        
    """AffinCraftæ•°æ®çš„æ‰¹å¤„ç†å‡½æ•°"""        
            
    # è¿‡æ»¤æ— æ•ˆæ•°æ®        
    items = [item for item in items if item is not None and item['node_feat'].size(0) <= max_node]        
            
    if not items:        
        return None        
            
    max_node_num = max(item['node_feat'].size(0) for item in items)        
    max_edge_num = max(item['edge_feat'].size(0) for item in items)  
      
    # è®¡ç®—æœ€å¤§ masif ç‰¹å¾æ•°é‡  
    max_masif_features = max(  
        item['masif_features']['desc_straight'].size(0)   
        for item in items   
        if 'masif_features' in item and 'desc_straight' in item['masif_features']  
    )  
            
    # æ‰¹å¤„ç†ç‰¹å¾        
    node_feats = []        
    edge_feats = []        
    edge_indices = []        
    edge_masks = []  
    coords_list = []        
    attn_biases = []        
    in_degrees = []        
    out_degrees = []        
    angles = []  
    dists_list = []  
    masif_desc_straights = []  # masif ç‰¹å¾åˆ—è¡¨  
    masif_masks = []  # masif æ©ç åˆ—è¡¨  
            
    for item in items:        
        n_node = item['node_feat'].size(0)        
        n_edge = item['edge_feat'].size(0)        
                
        # å¡«å……èŠ‚ç‚¹ç‰¹å¾        
        padded_node_feat = torch.zeros(max_node_num, item['node_feat'].size(1))        
        padded_node_feat[:n_node] = item['node_feat']        
        node_feats.append(padded_node_feat)        
                
        # å¡«å……è¾¹ç‰¹å¾  
        padded_edge_feat = torch.zeros(max_edge_num, item['edge_feat'].size(1))        
        padded_edge_feat[:n_edge] = item['edge_feat']        
        edge_feats.append(padded_edge_feat)        
                
        # å¡«å……è¾¹ç´¢å¼•        
        padded_edge_index = torch.zeros(2, max_edge_num, dtype=torch.long)        
        padded_edge_index[:, :n_edge] = item['edge_index']        
        edge_indices.append(padded_edge_index)        
                
        # åˆ›å»ºè¾¹æ©ç         
        edge_mask = torch.zeros(max_edge_num, dtype=torch.bool)        
        edge_mask[:n_edge] = True        
        edge_masks.append(edge_mask)        
                
        # å…¶ä»–ç‰¹å¾å¤„ç†ä¿æŒä¸å˜        
        padded_coords = torch.zeros(max_node_num, 3)        
        padded_coords[:n_node] = item['coords']        
        coords_list.append(padded_coords)        
                
        padded_attn_bias = torch.zeros(max_node_num + 1, max_node_num + 1)        
        padded_attn_bias[:n_node+1, :n_node+1] = item['attn_bias']        
        attn_biases.append(padded_attn_bias)        
                
        padded_in_degree = torch.zeros(max_node_num, dtype=torch.long)        
        padded_in_degree[:n_node] = item['in_degree']        
        in_degrees.append(padded_in_degree)        
                
        padded_out_degree = torch.zeros(max_node_num, dtype=torch.long)        
        padded_out_degree[:n_node] = item['out_degree']        
        out_degrees.append(padded_out_degree)        
              
        if 'angle' in item and item['angle'] is not None:  
            angle_size = item['angle'].size(0)  # ä½¿ç”¨angleè‡ªèº«çš„å¤§å°  
            padded_angle = torch.zeros(max_node_num, max_node_num, item['angle'].size(-1))  
            padded_angle[:angle_size, :angle_size] = item['angle']  
            angles.append(padded_angle)  
              
            padded_dists = torch.zeros(max_node_num, max_node_num, item['dists'].size(-1))  
            padded_dists[:angle_size, :angle_size] = item['dists']  
            dists_list.append(padded_dists)  
        else:  
            angles.append(None)  
            dists_list.append(None)
              
        # å¤„ç† masif ç‰¹å¾ - æ–°å¢å¡«å……å’Œæ©ç å¤„ç†  
        if 'masif_features' in item and 'desc_straight' in item['masif_features']:  
            masif_feat = item['masif_features']['desc_straight']  
            n_masif = masif_feat.size(0)  
              
            # å¡«å……ç‰¹å¾åˆ°æœ€å¤§é•¿åº¦  
            padded_masif = torch.zeros(max_masif_features, masif_feat.size(1))  
            padded_masif[:n_masif] = masif_feat  
            masif_desc_straights.append(padded_masif)  
              
            # ç”Ÿæˆæ©ç ï¼šTrueè¡¨ç¤ºæœ‰æ•ˆç‰¹å¾ï¼ŒFalseè¡¨ç¤ºå¡«å……  
            masif_mask = torch.zeros(max_masif_features, dtype=torch.bool)  
            masif_mask[:n_masif] = True  
            masif_masks.append(masif_mask)  
        else:  
            # å¦‚æœæ²¡æœ‰ masif ç‰¹å¾ï¼Œåˆ›å»ºé›¶å¼ é‡å’Œå…¨Falseæ©ç   
            masif_desc_straights.append(torch.zeros(max_masif_features, 80))  
            masif_masks.append(torch.zeros(max_masif_features, dtype=torch.bool))  
      
    return {        
        'node_feat': torch.stack(node_feats),        
        'edge_feat': torch.stack(edge_feats),  
        'edge_index': torch.stack(edge_indices),  
        'edge_mask': torch.stack(edge_masks),  
        'coords': torch.stack(coords_list),        
        'attn_bias': torch.stack(attn_biases),        
        'in_degree': torch.stack(in_degrees),        
        'out_degree': torch.stack(out_degrees),        
        'num_ligand_atoms': torch.tensor([item['num_ligand_atoms'] for item in items]),        
        'gbscore': torch.stack([item['gbscore'] for item in items]),        
        'masif_desc_straight': torch.stack(masif_desc_straights),  # å¡«å……åçš„ masif ç‰¹å¾  
        'masif_mask': torch.stack(masif_masks),  # æ–°å¢ï¼šmasif ç‰¹å¾æ©ç   
        'pdbid': [item['pdbid'] for item in items],        
        'pk': torch.tensor([item['pk'] for item in items]),        
        'smiles': [item['smiles'] for item in items],      
        'rmsd': torch.tensor([item['rmsd'] for item in items], dtype=torch.float),      
        'angle': torch.stack([a for a in angles if a is not None]) if any(a is not None for a in angles) else None,        
        'dists': torch.stack([d for d in dists_list if d is not None]) if any(d is not None for d in dists_list) else None,        
    }
  
  
# def create_affincraft_dataloader(pkl_files, batch_size=16, shuffle=True):    
#     """åˆ›å»ºAffinCraftæ•°æ®åŠ è½½å™¨"""    
#     dataset = AffinCraftDataset(pkl_files)    
        
#     return torch.utils.data.DataLoader(    
#         dataset,    
#         batch_size=batch_size,    
#         shuffle=shuffle,    
#         collate_fn=affincraft_collator,    
#         num_workers=8    
#     )
